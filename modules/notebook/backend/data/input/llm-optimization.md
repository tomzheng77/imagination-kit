# Optimizing LLM

#### How do I train LLaMA 3 to have a 16k context window?
- DHCCC
- need to run a first iteration with unsloth

#### Direct Context Window Editing
- context with dart2
- up to 6000 words
- one-shot training with response
- build a bot that responds for this

#### The idea is to have one strong train of thought inside the notes, instead of having lots of smaller train of thoughts
